<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-05-23T13:57:51+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Wen Laiâ€™s Blog</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>Wen Lai</name></author><entry><title type="html">Survey on Low-Resource Machine Translation</title><link href="http://localhost:4000/paper-notes/2022/05/23/Survey-of-Low-Resource-Machine-Translation.html" rel="alternate" type="text/html" title="Survey on Low-Resource Machine Translation" /><published>2022-05-23T00:00:00+02:00</published><updated>2022-05-23T00:00:00+02:00</updated><id>http://localhost:4000/paper-notes/2022/05/23/Survey-of-Low-Resource-Machine-Translation</id><content type="html" xml:base="http://localhost:4000/paper-notes/2022/05/23/Survey-of-Low-Resource-Machine-Translation.html"><![CDATA[<h3 id="survey-on-low-resource-machine-translation">Survey on Low-Resource Machine Translation</h3>

<hr />

<p>In this post, I would like to introduce a survey paper titled <a href="https://arxiv.org/pdf/2109.00486.pdf">Survey of Low-Resource Machine Translation</a>  from arrive, which is writen by the team of <em>University of Edinburgh</em>.</p>

<p>Current Machine Translation (MT) model are typically trained on data sets consisting of tens or even hundreds of millions of parallel sentences. Data sets of this magnitude are only available for a small number of highly resourced language pairs (typically English paired with some European languages, Arabic and Chinese). The reality is that for the vast majority of language pairs in the world the amount of data available is extremely limited, or simply non-existent. We called this language with limited resources as low-resource languages and the low-resource language MT research is chanllenging.</p>

<p>There are currently a number of other survey papers in related areas, for example a survey of monolingual data in low-resource NMT (<a href="https://arxiv.org/abs/1910.00373">Gibadullin et al., 2019</a>) and a survey of multilingual NMT (<a href="https://arxiv.org/abs/2001.01115">Dabre et al., 2020</a>). There have also been two very related surveys of low-resource MT (<a href="https://arxiv.org/abs/2106.15115">Ranathunga et al., 2021</a>, <a href="https://arxiv.org/abs/2107.04239">Wang et al., 2021</a>). I highly recommend interested readers to read the relevant survey papers above for a more in-depth understanding of the research on machine translation.</p>

<p>In this survey, the authors aims to provide the broadest coverage of existing research in the field and they also contribute an extensive overview of the tools and techniques validated across 18 low-resource shared tasks that ran between 2018 and 2021. The following figure shows how we structure the diverse research methods addressing low-resource MT, and this paper follows this structure.</p>

<p><img src="/assets/paper-notes/2022-05-23-Survey-of-Low-Resource-Machine-Translation.assets/approach.png" alt="approach" /></p>

<center><b>Overview of research methods covered in this survey</b> (Haddow et al., 2021)</center>

<p>In <strong>data collection</strong> part, They discuss the commonly used data sets and how to extract more data we can use to train the MT model from website. Also they summarize some useful sources of monolingual, paralllel  and benchmarking test data in Table 2 of the original paper, please go to the paper to see the specific summaries.</p>

<p>For low-resource language pairs, parallel text is, may not yield sufficient text to train high-quality MT models. However, monolingual text will almost always be more plentiful than parallel, and leveraging monolingual data has therefore been one of the most important and successful areas of research in low-resource MT. In <strong>monlingual</strong> part, they provide an overview of the main approaches used to exploit monolingual data in low-resource scenarios. They focused on the following three questions:</p>

<ul>
  <li>How to integrating external language models into NMT?</li>
  <li>How to using synthesis parallel data generate from monolingual data?</li>
  <li>How can we do transfer learning using monolingual data only?</li>
</ul>

<p>The use of monolingual data for low-resource has previously been surveyed by <a href="https://arxiv.org/abs/1910.00373">Gibadullin et al., 2019</a>, who choose to categorise methods according to whether they are architecture-independent or architecture-dependent. We recommend the readers to read this paper to understand the monolingual data using technologies in low-resource MT.</p>

<p>In <strong>multilingual data</strong> part, they consider a different but related set of methods, which use additional data from different languages (i.e. in languages other than the language pair that we consider for translation). These multilingual approaches can be roughly divided into two categories: (i) transfer learning and (ii) multilingual models.</p>

<p>In <strong>other resources</strong> part, they first introduce some linguistic tools and resources using in low-resource MT, such as morphological segmentation, factored models, multi-task learning, interleaving of linguistic information in the input and syntactic reordering. Then they discuss the techniques of using bilingual lexicon in low-resource MT scenarios.</p>

<p>In <strong>model techniques</strong> part, they explore work that aims to make better use of the data we already have by investigating better modelling, training and inference techniques, which contains the following techniques:</p>

<ul>
  <li><strong>Meta-Learning</strong>. rather than training a system to directly perform well on a single task or fixed set of tasks (language pairs in our case), meta-learning  systems can be trained to quickly adapt to a novel task using only a small number of training examples, as long as this task is sufficiently similar to tasks seen during (meta-)training.</li>
  <li><strong>Latent variable models</strong>. Latent variable models enable a higher model expressivity and more freedom in the engineering of the inductive biases, at the cost of more complicated and computa- tionally expensive training and inference.</li>
  <li><strong>Alternative training objectives</strong>. When an autoregressive model is trained to optimise the cross-entropy loss, it is only exposed to ground-truth examples during training. When this model is then used to generate a sequence with ancestral sampling, beam search or another inference method, it has to incrementally extend a prefix that it has generated itself.</li>
  <li><strong>Alternative inference algorithms</strong>. In NMT, inference is typically performed using a type of beam search algorithm with heuristic length normalisation. Ostensibly, beam search seeks to approximate maximum a posteriori (MAP) inference, however it has been noticed that increasing the beam size, which improves the accuracy of the approximation, often degrades translation quality after a certain point.</li>
  <li><strong>Rule-based approaches</strong>. Rule-based machine translation (RBMT) consists in analysing and transforming a source text into a translation by applying a set of hand-coded linguistically motivated rules.</li>
</ul>

<p>In <strong>evaluate</strong> part, they introduce the concept of manual or automatic evaluation.</p>

<p>In <strong>shared task</strong> part, they survey the shared tasks that have included low-resource language pairs, and  draw common themes from the corresponding sets of system description papers, putting into perspective the methods previously described in this survey. The following table contains the shared tasks related low-reousce MT:</p>

<p><img src="/assets/paper-notes/2022-05-23-Survey-of-Low-Resource-Machine-Translation.assets/shared_task.png" alt="shared_task" /></p>

<center><b>Shared tasks that have included low-resource language pairs.</b> (Haddow et al., 2021)</center>

<p>And also, they introduce the commonly used techniques in the shared tasks containing:</p>

<ul>
  <li>Data preparation</li>
  <li>Data pre-processing</li>
  <li>Model architectures and training</li>
  <li>Using additional data</li>
  <li>Model transformation and finalisation</li>
</ul>

<hr />

<p>In summary, this is a survey paper for those reseachers intersting in low-resource MT. we recommend the readers to read more deepth in this paper. For a summary notes and reference paper in this paper, we have writen more details in a mind map <a href="https://github.com/lavine-lmu/nlp_survey_paper_notes/blob/main/MT/loresMT/Low-Resource%20Machine%20Translation.xmind">here</a>.</p>]]></content><author><name>Wen Lai</name></author><category term="paper-notes" /><category term="MT" /><category term="Low-Resource" /><summary type="html"><![CDATA[Survey on Low-Resource Machine Translation]]></summary></entry></feed>